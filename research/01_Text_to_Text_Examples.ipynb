{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to Text Generation Examples using Gemini Pro Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isham993/Desktop/Programming-Tutorials/2023-Data-Science/google-gemini-project/getting-started-with-gemini-models-demo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isham993/mambaforge/envs/google_gemini_environment2/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Google AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "['generateMessage', 'countMessageTokens']\n",
      "models/text-bison-001\n",
      "['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "models/embedding-gecko-001\n",
      "['embedText', 'countTextTokens']\n",
      "models/gemini-pro\n",
      "['generateContent', 'countTokens']\n",
      "models/gemini-pro-vision\n",
      "['generateContent', 'countTokens']\n",
      "models/embedding-001\n",
      "['embedContent', 'countTextTokens']\n",
      "models/aqa\n",
      "['generateAnswer']\n",
      "Successfully configured Google AI API.\n"
     ]
    }
   ],
   "source": [
    "configure_google_ai_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(GEMINI_PRO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.03 ms, sys: 19.1 ms, total: 26.1 ms\n",
      "Wall time: 7.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = model.generate_content(\"Explain recursion to a 5 years old.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion is like when you tell a story and the story has a smaller version of itself inside. It's like a puzzle where you keep finding smaller puzzles inside the big one.\n",
      "\n",
      "Imagine you have a big box of building blocks. You take some blocks and build a tower. Then, you take some more blocks and build a smaller tower on top of the first one. And you keep going, building smaller and smaller towers on top of each other.\n",
      "\n",
      "Recursion is like that. You start with a problem, and then you break it down into smaller parts. Each smaller part is like a smaller version of the original problem. And you keep breaking it down until you reach a part that's easy to solve. Then, you work your way back up, solving each smaller part until you've solved the original problem.\n",
      "\n",
      "For example, let's say you want to play a game of hide-and-seek. You have to count to 100 before you can start looking for your friends.\n",
      "\n",
      "You could count like this:\n",
      "\n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
      "\n",
      "But you could also count like this:\n",
      "\n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
      "11, 12, 13, 14, 15, 16, 17, 18, 19, 20\n",
      "21, 22, 23, 24, 25, 26, 27, 28, 29, 30\n",
      "\n",
      "And so on.\n",
      "\n",
      "This is recursion. You're breaking a problem down into smaller parts (counting from 1 to 10), and then you're using those parts to solve the original problem (counting to 100).\n",
      "\n",
      "Recursion is a very powerful tool that can be used to solve many different kinds of problems. It's like having a magic wand that can make your problems disappear!\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.01 ms, sys: 9.5 ms, total: 12.5 ms\n",
      "Wall time: 5.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = model.generate_content(\"Write a poem on Monkey D. Luffy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world of pirates, a legend takes flight,\n",
      "Monkey D. Luffy, with dreams burning bright.\n",
      "From the East Blue he sails, with a heart full of grace,\n",
      "Seeking adventures, at a relentless pace.\n",
      "\n",
      "With a straw hat atop, his spirit soars high,\n",
      "A captain he yearns to be, under the vast blue sky.\n",
      "A crew he gathers, each with their own might,\n",
      "Sailing the seas, defying the night.\n",
      "\n",
      "Through storms and trials, they face every test,\n",
      "With unwavering resolve, they always stand abreast.\n",
      "With a devilish grin, and a fist raised bold,\n",
      "Luffy charges forth, his story yet untold.\n",
      "\n",
      "He dreams of One Piece, a treasure untold,\n",
      "A legacy left by the Pirate King of old.\n",
      "With every island they reach, new wonders unfold,\n",
      "As Luffy's spirit continues to unfold.\n",
      "\n",
      "His rubbery body, a power unique,\n",
      "Grants him agility, with every strike.\n",
      "With his Gum-Gum powers, he battles the strong,\n",
      "Defying the odds, he proves he belongs.\n",
      "\n",
      "In the face of adversity, he never gives in,\n",
      "With unwavering spirit, he always wins.\n",
      "Through laughter and tears, he forges strong ties,\n",
      "A brotherhood of pirates, that never dies.\n",
      "\n",
      "So let the seas roar, and the winds howl,\n",
      "Monkey D. Luffy, never backs down.\n",
      "With a smile on his face, and a heart full of cheer,\n",
      "He sails the Grand Line, his destiny near.\n",
      "\n",
      "In this pirate's tale, adventure's the key,\n",
      "As Luffy pursues his dreams, with relentless glee.\n",
      "For in the world of One Piece, where legends reside,\n",
      "Monkey D. Luffy's spirit, shall never subside.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the safety ratings of the response using attribute `.prompt_feedback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HATE_SPEECH\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HARASSMENT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "  probability: NEGLIGIBLE\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.prompt_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from general generation config such as temperature, max_tokens etc, we can even configure safety settings of the model. \n",
    "\n",
    "Details can be found here: https://ai.google.dev/docs/safety_setting_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "generation_config = {\n",
    "  \"temperature\": 0.9,\n",
    "  \"top_p\": 1,\n",
    "  \"top_k\": 1,\n",
    "  \"max_output_tokens\": 4096,\n",
    "}\n",
    "\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name=GEMINI_PRO,\n",
    "                              generation_config=generation_config,\n",
    "                              safety_settings=safety_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also, prompt the model in parts as a list. This technique is known as Chain of Verification and is one of the ways to reduce hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monaco is the most densely populated country, with a population density of 18,678 people per square kilometer (2020).  The capital of Monaco is Monaco.\n"
     ]
    }
   ],
   "source": [
    "prompt_parts = [\n",
    "  \"Which is the most densely populated country?\",\n",
    "  \"What is the capital of that country?\",\n",
    "]\n",
    "\n",
    "\n",
    "response = model.generate_content(prompt_parts)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HATE_SPEECH\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HARASSMENT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "  probability: NEGLIGIBLE\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.prompt_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also allows to save the conversation in form of json internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name=GEMINI_PRO)\n",
    "\n",
    "\n",
    "chat = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.generativeai.types.generation_types.GenerateContentResponse at 0x11c47d790>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.send_message(\"Who is the highest paid actor in Bangladesh?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the response using `chat.last.text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shakib Khan'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.last.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.generativeai.types.generation_types.GenerateContentResponse at 0x11c6f8cd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.send_message(\"Who is the highest paid actor in India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Akshay Kumar'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.last.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the whole history using `chat.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Who is the highest paid actor in Bangladesh?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Shakib Khan\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Who is the highest paid actor in India?\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Akshay Kumar\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_experiment_mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
